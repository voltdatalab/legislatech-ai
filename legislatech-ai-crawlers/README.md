# Legislatech AI Crawlers

[![Python](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Playwright](https://img.shields.io/badge/Playwright-Automation-green.svg)](https://playwright.dev/)
[![BeautifulSoup](https://img.shields.io/badge/BeautifulSoup-Web%20Scraping-orange.svg)](https://www.crummy.com/software/BeautifulSoup/)
[![Spacy](https://img.shields.io/badge/Spacy-NLP-purple.svg)](https://spacy.io/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)

> **Sistema de crawlers especializados para coleta automatizada de legislaÃ§Ã£o brasileira** - Desenvolvido com Playwright, BeautifulSoup e IA

## ðŸš€ Sobre o Projeto

O **Legislatech AI Crawlers** Ã© um sistema modular de web scraping especializado na coleta automatizada de legislaÃ§Ã£o brasileira. Utilizando tecnologias avanÃ§adas de automaÃ§Ã£o web e processamento de linguagem natural, extraÃ­mos, processamos e estruturamos dados legislativos de fontes oficiais.

### ðŸŽ¯ Objetivos

- **Automatizar a coleta** de legislaÃ§Ã£o de fontes oficiais
- **Estruturar dados** legislativos de forma consistente
- **Processar metadados** com IA (datas, nomes, entidades)
- **Manter histÃ³rico** completo de alteraÃ§Ãµes legislativas
- **Garantir qualidade** dos dados coletados

### ðŸŒŸ Diferenciais

- **AutomaÃ§Ã£o Inteligente**: Playwright para sites dinÃ¢micos
- **Processamento de IA**: Reconhecimento de nomes e datas
- **Modularidade**: Crawlers especializados por tipo de legislaÃ§Ã£o
- **Robustez**: Tratamento de erros e retry automÃ¡tico
- **Escalabilidade**: Processamento paralelo com workers

## ðŸ“‹ Tipos de LegislaÃ§Ã£o Suportados

### ðŸ“œ LegislaÃ§Ã£o Federal
- **Leis OrdinÃ¡rias**: Leis comuns do Congresso Nacional
- **Leis Complementares**: Leis que complementam a ConstituiÃ§Ã£o
- **Leis Delegadas**: Leis delegadas pelo Congresso ao Presidente
- **Medidas ProvisÃ³rias**: MPs editadas pelo Presidente
- **Decretos**: Atos normativos do Presidente
- **Emendas Constitucionais**: ModificaÃ§Ãµes na ConstituiÃ§Ã£o

### ðŸ›ï¸ Ã“rgÃ£os EspecÃ­ficos
- **ANTT**: AgÃªncia Nacional de Transportes Terrestres
- **BCB**: Banco Central do Brasil
- **ANA**: AgÃªncia Nacional de Ãguas

### ðŸ“š CÃ³digos e Estatutos
- **CÃ³digo de Defesa do Consumidor**: Lei 8.078/1990
- **HistÃ³rico de CÃ³digos**: EvoluÃ§Ã£o dos cÃ³digos brasileiros
- **HistÃ³rico de ConstituiÃ§Ãµes**: ConstituiÃ§Ãµes histÃ³ricas
- **Estatutos**: Estatutos especÃ­ficos

### ðŸ“„ Documentos Especiais
- **Mensagens de Veto**: Veto total ou parcial do Presidente
- **ResoluÃ§Ãµes ANA**: ResoluÃ§Ãµes da AgÃªncia Nacional de Ãguas
- **LegislaÃ§Ã£o e PublicaÃ§Ã£o**: Documentos de publicaÃ§Ã£o oficial

## ðŸ—ï¸ Arquitetura

### VisÃ£o Geral do Sistema Legislatech AI

O **Legislatech AI** Ã© um ecossistema completo de inteligÃªncia artificial para anÃ¡lise de legislaÃ§Ã£o brasileira, composto por trÃªs mÃ³dulos principais:

```
legislatech-ai/
â”œâ”€â”€ legislatech-ai-api/           # API REST com FastAPI
â”‚   â”œâ”€â”€ routes/                  # Endpoints organizados por versÃ£o
â”‚   â”‚   â”œâ”€â”€ v1.py               # RAG Search com reranking
â”‚   â”‚   â”œâ”€â”€ v2.py               # Intent Router
â”‚   â”‚   â”œâ”€â”€ v3.py               # Generic Search
â”‚   â”‚   â””â”€â”€ grafo.py            # Grafo Crawler
â”‚   â”œâ”€â”€ utils/                   # UtilitÃ¡rios e helpers
â”‚   â”‚   â”œâ”€â”€ auth.py             # AutenticaÃ§Ã£o segura
â”‚   â”‚   â”œâ”€â”€ prompt_helpers.py   # Helpers de prompts
â”‚   â”‚   â””â”€â”€ sse.py              # Server-Sent Events
â”‚   â”œâ”€â”€ config.py               # ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ main.py                 # AplicaÃ§Ã£o principal
â”‚   â”œâ”€â”€ intent_router.py        # Roteamento inteligente
â”‚   â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”‚   â”œâ”€â”€ Dockerfile             # ContainerizaÃ§Ã£o
â”‚   â””â”€â”€ README.md              # DocumentaÃ§Ã£o da API
â”œâ”€â”€ legislatech-ai-crawlers/     # Sistema de Crawlers
â”‚   â”œâ”€â”€ leis-ordinarias/        # Crawler de Leis OrdinÃ¡rias
â”‚   â”œâ”€â”€ decretos-emendas/       # Crawler de Decretos e Emendas
â”‚   â”œâ”€â”€ medidas-provisorias/    # Crawler de Medidas ProvisÃ³rias
â”‚   â”œâ”€â”€ antt/                   # Crawler da ANTT
â”‚   â”œâ”€â”€ bcb/                    # Crawler do BCB
â”‚   â”œâ”€â”€ codigo-consumidor/      # Crawler do CDC
â”‚   â”œâ”€â”€ historico-codigos/      # Crawler de cÃ³digos histÃ³ricos
â”‚   â”œâ”€â”€ historico-constituicoes/ # Crawler de constituiÃ§Ãµes
â”‚   â”œâ”€â”€ historico-decretos/     # Crawler de decretos histÃ³ricos
â”‚   â”œâ”€â”€ historico-decretos-leis/ # Crawler de decretos-leis
â”‚   â”œâ”€â”€ historico-estatutos/    # Crawler de estatutos
â”‚   â”œâ”€â”€ legislacao-e-publicacao/ # Crawler de publicaÃ§Ãµes
â”‚   â”œâ”€â”€ leis-complementares/    # Crawler de leis complementares
â”‚   â”œâ”€â”€ leis-delegadas/         # Crawler de leis delegadas
â”‚   â”œâ”€â”€ resolucoes-ana/         # Crawler de resoluÃ§Ãµes ANA
â”‚   â”œâ”€â”€ mensagem-de-veto-total/ # Crawler de vetos
â”‚   â”œâ”€â”€ requirements.txt        # DependÃªncias globais
â”‚   â””â”€â”€ README.md              # Este arquivo
â””â”€â”€ README.md                   # DocumentaÃ§Ã£o principal
```

### Arquitetura dos Crawlers

Cada crawler segue uma estrutura modular padronizada:

```
crawler-especifico/
â”œâ”€â”€ main.py              # LÃ³gica principal do crawler
â”œâ”€â”€ agents.py            # Agentes de IA para processamento
â”œâ”€â”€ date_spacy/          # Pipeline de reconhecimento de datas
â”œâ”€â”€ dicionario_dados     # Estrutura de dados
â”œâ”€â”€ requirements.txt     # DependÃªncias especÃ­ficas
â”œâ”€â”€ Dockerfile          # ContainerizaÃ§Ã£o
â””â”€â”€ .env                # ConfiguraÃ§Ãµes de ambiente
```

### Fluxo de Dados

```
1. Fontes Oficiais
   â†“
2. Crawlers Especializados
   â”œâ”€â”€ Playwright (automaÃ§Ã£o web)
   â”œâ”€â”€ BeautifulSoup (parsing HTML)
   â””â”€â”€ Cloudscraper (bypass anti-bot)
   â†“
3. Processamento com IA
   â”œâ”€â”€ Reconhecimento de nomes (GPT-4)
   â”œâ”€â”€ ExtraÃ§Ã£o de datas (Spacy customizado)
   â””â”€â”€ SanitizaÃ§Ã£o de texto
   â†“
4. EstruturaÃ§Ã£o de Dados
   â”œâ”€â”€ Metadados padronizados
   â”œâ”€â”€ Texto processado
   â””â”€â”€ Relacionamentos
   â†“
5. Armazenamento
   â”œâ”€â”€ PostgreSQL + pgvector
   â”œâ”€â”€ Embeddings vetoriais
   â””â”€â”€ Ãndices otimizados
   â†“
6. API de Consulta
   â”œâ”€â”€ RAG Search
   â”œâ”€â”€ Intent Router
   â””â”€â”€ Generic Search
```

### Tecnologias por Camada

#### ðŸ•·ï¸ **Camada de Coleta**
- **Playwright**: AutomaÃ§Ã£o de navegadores para sites dinÃ¢micos
- **BeautifulSoup**: Parsing HTML e extraÃ§Ã£o de conteÃºdo
- **Cloudscraper**: Bypass de proteÃ§Ãµes anti-bot
- **Requests**: RequisiÃ§Ãµes HTTP assÃ­ncronas

#### ðŸ¤– **Camada de IA**
- **OpenAI GPT-4**: Reconhecimento de nomes e entidades
- **Spacy**: Processamento de linguagem natural
- **Pipeline Customizado**: Reconhecimento de datas em portuguÃªs
- **Agentes Especializados**: ValidaÃ§Ã£o e classificaÃ§Ã£o

#### ðŸ—„ï¸ **Camada de Dados**
- **PostgreSQL**: Banco de dados principal
- **pgvector**: ExtensÃ£o para busca vetorial
- **SQLAlchemy**: ORM e pool de conexÃµes
- **Pandas**: ManipulaÃ§Ã£o e anÃ¡lise de dados

#### ðŸ”§ **Camada de Processamento**
- **ThreadPoolExecutor**: Processamento paralelo
- **Retry Logic**: Tratamento de falhas
- **Rate Limiting**: Controle de requisiÃ§Ãµes
- **Logging**: Monitoramento detalhado

### IntegraÃ§Ã£o com a API

Os crawlers alimentam a API atravÃ©s de:

1. **Coleta ContÃ­nua**: Dados atualizados regularmente
2. **Estrutura Padronizada**: Formato consistente para todos os tipos
3. **Metadados Enriquecidos**: InformaÃ§Ãµes processadas com IA
4. **Relacionamentos**: ConexÃµes entre documentos legais

### Escalabilidade

#### Horizontal
- **Crawlers Independentes**: Cada tipo de legislaÃ§Ã£o em container separado
- **Workers Paralelos**: Processamento simultÃ¢neo de mÃºltiplos documentos
- **Load Balancing**: DistribuiÃ§Ã£o de carga entre instÃ¢ncias

#### Vertical
- **OtimizaÃ§Ã£o de MemÃ³ria**: Gerenciamento eficiente de recursos
- **Cache Inteligente**: ReduÃ§Ã£o de requisiÃ§Ãµes desnecessÃ¡rias
- **CompressÃ£o de Dados**: Armazenamento otimizado

### Monitoramento e Observabilidade

#### MÃ©tricas Coletadas
- **Taxa de Sucesso**: % de pÃ¡ginas coletadas com sucesso
- **Tempo de Processamento**: LatÃªncia por documento
- **Uso de Recursos**: CPU, memÃ³ria, rede
- **Qualidade dos Dados**: PrecisÃ£o das extraÃ§Ãµes

#### Logs Estruturados
- **NÃ­vel de AplicaÃ§Ã£o**: OperaÃ§Ãµes dos crawlers
- **NÃ­vel de Sistema**: Recursos e performance
- **NÃ­vel de NegÃ³cio**: Dados coletados e processados

### SeguranÃ§a

#### ProteÃ§Ãµes Implementadas
- **Rate Limiting**: Controle de velocidade de requisiÃ§Ãµes
- **User-Agent Rotation**: VariaÃ§Ã£o de identificadores
- **Proxy Support**: Suporte a proxies para anonimizaÃ§Ã£o
- **Error Handling**: Tratamento seguro de falhas

#### Boas PrÃ¡ticas
- **Respeito aos robots.txt**: Conformidade com diretrizes dos sites
- **Delays Inteligentes**: Pausas entre requisiÃ§Ãµes
- **Retry com Backoff**: Tentativas com intervalos crescentes
- **SanitizaÃ§Ã£o de Dados**: Limpeza de conteÃºdo malicioso

## ðŸ› ï¸ PrÃ©-requisitos

### Sistema
- **Python 3.12+** (recomendado)
- **8GB RAM** mÃ­nimo (16GB recomendado)
- **ConexÃ£o estÃ¡vel** com a internet
- **Docker** (opcional, para containerizaÃ§Ã£o)

### DependÃªncias Externas
- **Chave de API da OpenAI** (GPT-4 ou superior)
- **Acesso aos sites** oficiais do governo
- **Git** para clonagem

## ðŸš€ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio
```bash
git clone https://github.com/seu-usuario/legislatech-ai.git
cd legislatech-ai/legislatech-ai-crawlers
```

### 2. Configure o ambiente
```bash
# Crie ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate  # Windows

# Instale dependÃªncias globais
pip install -r requirements.txt
```

### 3. Configure as variÃ¡veis de ambiente
Crie um arquivo `.env` em cada crawler:

```env
# === ConfiguraÃ§Ãµes da OpenAI ===
OPENAI_API_KEY=sk-your-openai-api-key-here

# === ConfiguraÃ§Ãµes do Banco de Dados ===
DATABASE_URL=postgresql://usuario:senha@localhost:5432/legislatech

# === ConfiguraÃ§Ãµes do Crawler ===
WORKERS=10
TIMEOUT=60
RETRY_ATTEMPTS=3
DELAY_BETWEEN_REQUESTS=1

# === ConfiguraÃ§Ãµes de Log ===
LOG_LEVEL=INFO
SAVE_RAW_DATA=true
```

### 4. Instale o Playwright
```bash
# Instale os navegadores necessÃ¡rios
playwright install

# Ou instale apenas o Chromium
playwright install chromium
```

### 5. Configure o Spacy (se necessÃ¡rio)
```bash
# Baixe o modelo portuguÃªs (se usar)
python -m spacy download pt_core_news_sm

# Ou use o modelo customizado (jÃ¡ incluÃ­do)
# Os modelos customizados estÃ£o em date_spacy/
```

## ðŸ”§ Executando os Crawlers

### ExecuÃ§Ã£o Individual

#### Leis OrdinÃ¡rias
```bash
cd leis-ordinarias
python main.py
```

#### Decretos e Emendas
```bash
cd decretos-emendas
python main.py
```

#### Medidas ProvisÃ³rias
```bash
cd medidas-provisorias
python main.py
```

### ExecuÃ§Ã£o com Docker

#### Build da imagem
```bash
cd leis-ordinarias
docker build -t legislatech-crawler-leis .
```

#### ExecuÃ§Ã£o
```bash
docker run --env-file .env legislatech-crawler-leis
```

### ExecuÃ§Ã£o Paralela
```bash
# Execute mÃºltiplos crawlers simultaneamente
python -c "
import subprocess
import threading

crawlers = [
    'leis-ordinarias',
    'decretos-emendas', 
    'medidas-provisorias'
]

def run_crawler(crawler):
    subprocess.run(['python', 'main.py'], cwd=crawler)

threads = []
for crawler in crawlers:
    t = threading.Thread(target=run_crawler, args=(crawler,))
    threads.append(t)
    t.start()

for t in threads:
    t.join()
"
```

## ðŸ“Š Estrutura de Dados

### Formato PadrÃ£o dos Dados

```python
{
    # === Metadados BÃ¡sicos ===
    'num_lei': '12345',                    # NÃºmero da lei
    'data_lei': '2023-12-01',              # Data da lei
    'data_dou': '2023-12-02',              # Data publicaÃ§Ã£o DOU
    'ano': 2023,                           # Ano da legislaÃ§Ã£o
    
    # === Texto Processado ===
    'bleached_nome': 'Lei 12345 de 2023',  # Nome limpo
    'non_bleached_nome': 'Lei nÂº 12.345, de 1Âº de dezembro de 2023',  # Nome original
    'ementa': 'DispÃµe sobre...',           # Ementa da lei
    
    # === ConteÃºdo Completo ===
    'lei_text': 'Texto da lei processado...',           # Texto limpo
    'non_parsed_lei_text': 'Texto original...',         # Texto original
    'lei_text_striked': 'Texto revogado...',            # Texto revogado
    'full_lei_text': 'Texto completo...',               # Texto completo
    
    # === Metadados AvanÃ§ados ===
    'sancionadores': ['JoÃ£o Silva', 'Maria Santos'],     # Nomes extraÃ­dos
    'link': 'https://www.planalto.gov.br/...',          # URL original
    'crawled_at': '2023-12-01T10:00:00Z'                # Timestamp
}
```

### Campos EspecÃ­ficos por Tipo

#### Leis OrdinÃ¡rias
- `num_lei`: NÃºmero da lei
- `data_lei`: Data de sanÃ§Ã£o
- `data_dou`: Data de publicaÃ§Ã£o no DOU

#### Medidas ProvisÃ³rias
- `num_mp`: NÃºmero da MP
- `data_edicao`: Data de ediÃ§Ã£o
- `data_aprovacao`: Data de aprovaÃ§Ã£o pelo Congresso

#### Decretos
- `num_decreto`: NÃºmero do decreto
- `data_decreto`: Data do decreto
- `tipo_decreto`: Tipo (executivo, legislativo, etc.)

## ðŸ¤– Processamento com IA

### Reconhecimento de Nomes

O sistema utiliza agentes de IA para extrair e validar nomes de pessoas:

```python
from agents import run_name_recognizer_agent

# Lista de possÃ­veis nomes extraÃ­dos do texto
possible_names = ['JoÃ£o da Silva', 'Casa de papel', 'Carlos Alberto']

# ValidaÃ§Ã£o com IA
valid_names = run_name_recognizer_agent(
    list_of_names=possible_names,
    model='gpt-4'
)

# Resultado: ['JoÃ£o da Silva', 'Carlos Alberto']
```

### Reconhecimento de Datas

Pipeline customizado do Spacy para extraÃ§Ã£o de datas:

```python
from date_spacy import find_dates

# Texto com datas
text = "Lei 12345 de 1Âº de dezembro de 2023"

# ExtraÃ§Ã£o de datas
nlp = spacy.blank("pt")
nlp.add_pipe('find_dates')
doc = nlp(text)

# Datas encontradas
dates = [ent._.date for ent in doc.ents]
```

### Processamento de Texto

```python
def sanitize_text(text: str) -> str:
    """Remove caracteres especiais e normaliza texto"""
    text = text.strip()
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')
    text = re.sub(r'\s+', ' ', text)
    return text
```

## ðŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Performance

#### ConfiguraÃ§Ã£o de Workers
```python
# Em main.py
def parse_link_with_workers(self, data_rows: list, year: int, workers: int = 10):
    """Processa links em paralelo com mÃºltiplos workers"""
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = []
        for data_row in data_rows:
            future = executor.submit(self.parse_link, [data_row], None, year)
            futures.append(future)
        
        for future in as_completed(futures):
            result = future.result()
```

#### ConfiguraÃ§Ã£o de Timeout
```python
# Timeout para requisiÃ§Ãµes
TIMEOUT = 60  # segundos

# Delay entre requisiÃ§Ãµes
DELAY_BETWEEN_REQUESTS = 1  # segundo

# NÃºmero de tentativas
RETRY_ATTEMPTS = 3
```

### Tratamento de Erros

#### Retry AutomÃ¡tico
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def fetch_with_retry(self, url: str) -> Response:
    """Faz requisiÃ§Ã£o com retry automÃ¡tico"""
    return self.session.get(url, timeout=TIMEOUT)
```

#### Logging Detalhado
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log'),
        logging.StreamHandler()
    ]
)
```

## ðŸ› Troubleshooting

### Problemas Comuns

#### 1. Erro de Playwright
```bash
# Reinstale o Playwright
playwright install --force

# Verifique se o Chromium estÃ¡ instalado
playwright install chromium
```

#### 2. Erro de Rate Limiting
```bash
# Aumente o delay entre requisiÃ§Ãµes
export DELAY_BETWEEN_REQUESTS=5

# Reduza o nÃºmero de workers
export WORKERS=5
```

#### 3. Erro de MemÃ³ria
```bash
# Reduza o nÃºmero de workers
export WORKERS=3

# Aumente a memÃ³ria do Python
export PYTHONMALLOC=malloc
```

#### 4. Erro de ConexÃ£o
```bash
# Verifique a conectividade
curl -I https://www.planalto.gov.br

# Teste com proxy (se necessÃ¡rio)
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port
```

### Logs Ãšteis

#### Monitoramento de Progresso
```bash
# Logs em tempo real
tail -f crawler.log

# EstatÃ­sticas de progresso
grep "Processed" crawler.log | tail -10

# Erros encontrados
grep "ERROR" crawler.log
```

#### VerificaÃ§Ã£o de Dados
```python
# Verifique os dados coletados
import pandas as pd

df = pd.read_csv('output/leis_ordinarias.csv')
print(f"Total de leis coletadas: {len(df)}")
print(f"Ãšltima atualizaÃ§Ã£o: {df['crawled_at'].max()}")
```

## ðŸš€ Deploy em ProduÃ§Ã£o

### Docker Compose
```yaml
version: '3.8'

services:
  crawler-leis:
    build: ./leis-ordinarias
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - WORKERS=5
    volumes:
      - ./data:/app/data
    restart: unless-stopped

  crawler-decretos:
    build: ./decretos-emendas
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - WORKERS=5
    volumes:
      - ./data:/app/data
    restart: unless-stopped

  scheduler:
    image: mcuadros/ofelia:latest
    command: daemon --docker
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:rw
      - ./scheduler.ini:/etc/ofelia/config.ini
    restart: unless-stopped
```

### Agendamento com Cron
```bash
# Adicione ao crontab
0 2 * * * cd /path/to/legislatech-ai-crawlers/leis-ordinarias && python main.py
0 3 * * * cd /path/to/legislatech-ai-crawlers/decretos-emendas && python main.py
0 4 * * * cd /path/to/legislatech-ai-crawlers/medidas-provisorias && python main.py
```

### Monitoramento
```bash
# Script de monitoramento
#!/bin/bash
for crawler in leis-ordinarias decretos-emendas medidas-provisorias; do
    if [ ! -f "/tmp/${crawler}_last_run" ]; then
        echo "Crawler ${crawler} nÃ£o executou hoje"
        # Enviar alerta
    fi
done
```

## ðŸ“ˆ MÃ©tricas e Performance

### Indicadores de Qualidade
- **Taxa de sucesso**: > 95% de pÃ¡ginas coletadas
- **PrecisÃ£o de dados**: > 98% de campos corretos
- **Tempo de processamento**: < 2 horas para 1000 documentos
- **Uso de memÃ³ria**: < 4GB por crawler

### OtimizaÃ§Ãµes Implementadas
- **Processamento paralelo** com ThreadPoolExecutor
- **Cache de sessÃ£o** para evitar reautenticaÃ§Ã£o
- **Retry inteligente** com backoff exponencial
- **SanitizaÃ§Ã£o eficiente** de texto
- **CompressÃ£o de dados** para armazenamento

## ðŸ¤ Contribuindo

### Adicionando um Novo Crawler

1. **Crie a estrutura**
   ```bash
   mkdir novo-crawler
   cd novo-crawler
   cp ../leis-ordinarias/* .
   ```

2. **Modifique main.py**
   ```python
   class Crawler:
       urls = {
           'main-page': 'https://site.oficial.gov.br',
       }
       
       def get_pages(self):
           # Implemente a lÃ³gica especÃ­fica
           pass
   ```

3. **Teste o crawler**
   ```bash
   python main.py --test
   ```

4. **Adicione Ã  documentaÃ§Ã£o**
   - Atualize este README
   - Documente campos especÃ­ficos
   - Adicione exemplos de uso

### Diretrizes de Desenvolvimento
- **Siga o padrÃ£o** dos crawlers existentes
- **Use type hints** em todas as funÃ§Ãµes
- **Adicione logs** detalhados
- **Implemente retry** para robustez
- **Teste com dados reais** antes do deploy

## ðŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](../LICENSE) para mais detalhes.

## ðŸ†˜ Suporte

### Canais de Ajuda
- **DocumentaÃ§Ã£o**: [docs.legisla.tech/crawlers](https://docs.legisla.tech/crawlers)
- **Email**: suporte@legisla.tech
- **Issues**: [GitHub Issues](https://github.com/seu-usuario/legislatech-ai/issues)
- **Discord**: [Comunidade Legislatech](https://discord.gg/legislatech)

### FAQ

**Q: Como adicionar um novo tipo de legislaÃ§Ã£o?**
A: Siga o template de leis-ordinarias e adapte para o novo tipo.

**Q: Como otimizar a performance?**
A: Ajuste o nÃºmero de workers e delay entre requisiÃ§Ãµes.

**Q: Como debugar problemas de scraping?**
A: Use logs detalhados e teste com `--debug` flag.

**Q: Como manter dados atualizados?**
A: Configure agendamento automÃ¡tico com cron ou Docker.

---

**Desenvolvido com â¤ï¸ pela equipe Legislatech**

*Automatizando a coleta de legislaÃ§Ã£o brasileira com inteligÃªncia artificial.*

### AnÃ¡lise de Relacionamentos Legais

```python
# AnÃ¡lise de grafo de uma lei especÃ­fica
response = requests.get("http://localhost:8000/grafo/", params={
    "urls": "https://www.planalto.gov.br/ccivil_03/leis/l8078.htm",
    "profundidade": 2,
    "top_n": 20
})

print(response.json())
```
